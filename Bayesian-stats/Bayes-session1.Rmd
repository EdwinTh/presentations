---
title: "Bayesian Data Analysis Session 1"
author: "Edwin Thoen"
date: "10/2/2017"
output: 
  beamer_presentation
---

## Overview

**Session 1: Edwin**

What is Bayesian statistics? Theory and simple examples.

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
# devtools::install_github("edwinth/tennis")
library(tennis)
name_to_last_name <- function(df, col) {
  col_q <- enquo(col)
  df %>% 
    mutate(!!rlang::quo_name(col_q) := 
             strsplit(!!col_q, " ") %>% map_chr(2))
}
rr <- tennis::atp_matches %>% 
  tennis::find_matchup(c("Roger Federer", "Rafael Nadal")) %>% 
  name_to_last_name(winner_name) %>% 
  name_to_last_name(loser_name)

ace_set <- atp_matches %>% 
  find_player("Roger Federer", "Rafael Nadal", "Novak Djokovic", "Ivo Karlovic", "John Isner") %>% 
  name_to_last_name(winner_name) %>% 
  name_to_last_name(loser_name) 
  
# write_csv(rr, "./rr.csv")
rr <- read_csv("./rr.csv")
source("ggmm.R")
```


**Session 2: Rick**

Building hierarchical models with Stan.

# Introduction

## Intuition of Bayesian Statistics

Statistics describes the world in probality distributions. 

Collect data to learn about the distributions: $\hat\theta$

Reverse engineering of parameters producing the data.

How do we deal with uncertainty?

A Bayesian:

- Sets a probabilty distribution on all $\theta$.
- Updates his beliefs with data.

## Set a prior: $P(\theta)$

```{r, echo = FALSE}
plot_data <- data_frame(theta = seq(0, 1, by = .001),
                        prior = dbeta(theta, 4, 4),
                        likelihood = theta^15 * (1-theta)^5,
                        posterior  = dbeta(theta, 18, 8))
settings <- function(gg_obj){
  gg_obj + geom_line() + 
  theme(axis.text.y  = element_blank(),
        axis.ticks.y = element_blank()) +
  ylab("")
}
ggplot(plot_data, aes(theta, prior)) %>% settings()
```

## Get the likelihood function: $P(D|\theta)$

```{r, echo = FALSE}
ggplot(plot_data, aes(theta, likelihood)) %>% settings()
```

## Update prior to posterior with likelihood: $P(\theta|D)$

```{r, echo=FALSE}
plot_data %>% select(-likelihood) %>% 
  gather(key = distribution, value = dens, -theta) %>% 
  ggplot(aes(theta, dens, col = distribution)) %>% settings()
```

Likelihood not in this plot, on different scale (why?).

## Bayesian data analysis

The essence of BDA is **credibility (re)allocation**.

We have an a priori idea about $\theta$:
  
- expert opinion
- previous research
- educated guess

Data provides evidence of the parameter value.

The posterior is a compromise between prior and likelihood. It reflects the current knowledge.

## Bayesian vs frequentist

- Frequentist only consider the likelihood.

- Frequentits have an objective view of probability. For Bayesians it is a subjective best guess.

- Frequentists: data random, parameters fixed. Bayesians: data fixed, parameters random.

## Why do we want BDA in the first place?

- Elegant and intuitive paradigm.

- Incorparation of previous knowledge and allowing for updating.

- Describe complex relationships without huge amounts of data.

# Probability

## Considered known

- sample space $\Omega$.

- probability functions.

- discrete and continuous random variables.

- expected value and variance of random variables.

- from probability distribution to likelihood.

## Probability of multiple events

```{r, echo = FALSE}
ggmm(rr, surface, winner_name)
```

## Joints, marginals and conditionals

(We assume the probabilities here as given, not as estimated.)

The joint is the probability two events coincide. $P(A = a \cap B = b)$ or for brevity $P(A \cap B)$

```{r, echo=FALSE}
joint <- rr %>% select(winner_name, surface) %>%
  table %>% prop.table() %>% round(3) %>% as.data.frame() %>% 
  spread(surface, Freq)
  
knitr::kable(joint)
```

## Joints, marginals and conditionals

The marginals are the univariate distributions of A and B. Sum over the other (or integrate them out when continuous).

```{r, echo = FALSE}
knitr::kable(joint[2:4] %>% colSums())
players <- joint[2:4] %>% rowSums()
names(players) <- c("Federer", "Nadal")
knitr::kable(players)
```

## Joints, marginals and conditionals

Conditionals, like $P(A|B)$, redefine $\Omega$, it is now a subset of the joint.

```{r, echo = FALSE}
ggmm(rr, surface, winner_name, surface == "Clay")
```

## Joints, marginals and conditionals

For $P(B|A)$ this looks 

```{r, echo = FALSE}
ggmm(rr, surface, winner_name, player == "Nadal")
```

## Joints, marginals and conditionals

With a continuous and discrete variable, a way to graph the data is

```{r, echo = FALSE, message=FALSE}
player_ace <- function(df, name) {
  df %>% 
    mutate(ace    = case_when(winner_name == name ~ w_ace, 
                              loser_name  == name ~ l_ace,
                              TRUE ~ NA_character_),
           player = name) %>% 
    select(player, ace) %>% 
    filter(!is.na(ace))
}

aces_set <- c("Nadal", "Federer", "Djokovic", "Isner", "Karlovic") %>% 
  map_df(player_ace, df = ace_set) %>%
  mutate(ace = as.integer(ace))

ggplot(aces_set, aes(ace)) +
  geom_histogram(aes(fill = player), binwidth = 1, col = "black") +
  xlim(c(-1, 45)) +
  ggtitle("Aces per match, for five players")
```
## Marginal for players

What does the marginal for the players look like?

```{r, echo = FALSE}
ggplot(aces_set, aes(player)) +
  geom_bar()
```




```{r}
ggplot(aces_set, aes(ace, player)) +
  ggridges::geom_density_ridges(aes(fill = player)) +
  guides(fill = FALSE) +
  xlim(c(-1, 45))
```

